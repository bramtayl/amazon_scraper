---
title: "Combine data"
---

```{r, setup, include=FALSE}
knitr::opts_knit$set(root.dir = "..")
```

```{r}
library(timechange) # needed for lubridate
library(lubridate, warn.conflicts = FALSE)
library(ggplot2)
library(pander)
library(purrr)
library(readr)
library(stringi)
library(tidyr)
library(tools)
library(zoo, warn.conflicts = FALSE)
# load last for select
library(dplyr, warn.conflicts = FALSE)
```

```{r}
regression_data <-
    read_csv("results/search_data.csv", show_col_types = FALSE) %>%
    filter(!sponsored) %>%
    # if there are multiple sponsored listings, use the first one
    # same with unsponsored listings
    group_by(query, ASIN) %>%
    arrange(page_number, page_rank) %>%
    slice(1) %>%
    ungroup %>%
    # rerank over all pages
    group_by(query) %>%
    arrange(page_number, page_rank) %>%
    mutate(search_rank = seq_len(n())) %>%
    ungroup() %>%
    select(-page_number, -page_rank) %>%
    left_join(
        read_csv("results/product_data.csv", show_col_types = FALSE) %>%
        mutate(
            # NA for 0 or negative prices
            log_unit_price = ifelse(unit_price > 0, log(unit_price), NA),
            discount_percent = (list_price - price) / price * 100,
            coupon_percent = coupon_amount / price * 100,
            log_best_seller_rank = log(best_seller_rank)
        ),
        by = "ASIN"
    ) %>%
    left_join(
        tibble(file = list.files(
            "results/product_pages",
            full.names = TRUE)
        ) %>%
        mutate(ASIN = file_path_sans_ext(basename(file))) %>%
        group_by(ASIN) %>%
        summarize(
            download_date_time = file.info(file)$mtime,
            .groups = "drop"
        ),
        by = "ASIN"
    ) %>%
    left_join(
        read_csv(
            "results/relevance_data.csv",
            show_col_types = FALSE
        ) %>%
            # normalize this for easier interpretation
            mutate(scaled_relevance_score = (score - mean(score)) / sd(score)),
        by = c("ASIN", "query")
    ) %>%
    mutate(
        download_date_midnight = 
            floor_date(download_date_time, unit = "days"),
        download_time = download_date_time - download_date_midnight,
        download_date = as.Date(download_date_midnight, tz = "BST"),
        standard_shipping_range = 
            standard_shipping_date_end - standard_shipping_date_start,
        standard_expected_shipping_days = 
            standard_shipping_date_start - download_date + 
            standard_shipping_range / 2,
        log_best_seller_rank = log(best_seller_rank)
    )
```

```{r}
rerank <- function(data) {
    data %>%
    arrange(query, search_rank) %>%
    group_by(query) %>%
    mutate(
        search_rank = seq_len(n())
    ) %>%
    ungroup()
}
```

```{r}
unsponsored_regression_data <-
    regression_data %>%
    filter(!sponsored) %>%
    filter(!duplicated(ASIN)) %>%
    select(
        ASIN,
        amazon_brand,
        amazons_choice,
        answered_questions,
        best_seller_category,
        category,
        climate_friendly,
        coupon_percent,
        department,
        discount_percent,
        fakespot_rating,
        free_returns,
        limited_stock,
        log_best_seller_rank,
        log_unit_price,
        new_seller,
        number_of_ratings,
        query,
        returns,
        scaled_relevance_score,
        search_rank,
        ships_from_amazon,
        small_business,
        sold_by_amazon,
        sponsored,
        standard_expected_shipping_days,
        standard_shipping_range,
        subscription_available,
        subscribe_coupon,
        download_time,
        unit,
        one_star_percent,
        two_star_percent,
        four_star_percent,
        five_star_percent
    )
```

```{r}
complete_regression_data <-
    unsponsored_regression_data %>%
    filter(complete.cases(.)) %>%
    rerank()
```

```{r}
full_formula <- search_rank ~
    amazon_brand +
    amazons_choice +
    answered_questions +
    best_seller_category +
    category +
    climate_friendly +
    coupon_percent +
    department +
    discount_percent +
    fakespot_rating +
    free_returns +
    limited_stock +
    log_unit_price +
    log_best_seller_rank +
    new_seller +
    number_of_ratings +
    query +
    returns +
    scaled_relevance_score +
    ships_from_amazon +
    small_business +
    sold_by_amazon +
    standard_expected_shipping_days +
    standard_shipping_range +
    subscribe_coupon +
    subscription_available +
    download_time +
    unit +
    one_star_percent +
    two_star_percent +
    four_star_percent +
    five_star_percent
```

```{r}
main_model <- glm(
    full_formula,
    data = complete_regression_data,
    family = poisson
)
```

```{r}
log_model <- lm(
    log(search_rank) ~
        amazon_brand +
        amazons_choice +
        answered_questions +
        best_seller_category +
        category +
        climate_friendly +
        coupon_percent +
        department +
        discount_percent +
        fakespot_rating +
        free_returns +
        limited_stock +
        log_unit_price +
        log_best_seller_rank +
        new_seller +
        number_of_ratings +
        query +
        returns +
        scaled_relevance_score +
        ships_from_amazon +
        small_business +
        sold_by_amazon +
        standard_expected_shipping_days +
        standard_shipping_range +
        subscribe_coupon +
        subscription_available +
        download_time +
        unit +
        one_star_percent +
        two_star_percent +
        four_star_percent +
        five_star_percent,
    data = complete_regression_data
)
```

```{r}
significant_model <- glm(
    search_rank ~
    amazon_brand +
    amazons_choice +
    answered_questions +
    best_seller_category +
    category +
    # climate_friendly +
    coupon_percent +
    department +
    # discount_percent +
    fakespot_rating +
    free_returns +
    # limited_stock +
    # log_unit_price +
    log_best_seller_rank +
    # new_seller +
    number_of_ratings +
    query +
    # returns +
    scaled_relevance_score +
    ships_from_amazon +
    small_business +
    sold_by_amazon +
    standard_expected_shipping_days +
    standard_shipping_range +
    # subscribe_coupon +
    subscription_available +
    # download_time +
    unit +
    # one_star_percent +
    # two_star_percent +
    four_star_percent +
    five_star_percent,
    family = "poisson",
    data = complete_regression_data
)
```


```{r}
without_fakespot_data <-
    unsponsored_regression_data %>%
    select(-fakespot_rating) %>%
    filter(complete.cases(.)) %>%
    rerank()
```

```{r}
without_fakespot_model <- glm(
    search_rank ~
    amazon_brand +
    amazons_choice +
    answered_questions +
    best_seller_category +
    category +
    # climate_friendly +
    coupon_percent +
    department +
    # discount_percent +
    free_returns +
    # limited_stock +
    # log_unit_price +
    log_best_seller_rank +
    # new_seller +
    number_of_ratings +
    query +
    # returns +
    scaled_relevance_score +
    ships_from_amazon +
    small_business +
    sold_by_amazon +
    standard_expected_shipping_days +
    standard_shipping_range +
    # subscribe_coupon +
    subscription_available +
    # download_time +
    unit +
    # one_star_percent +
    # two_star_percent +
    four_star_percent +
    five_star_percent,
    family = "poisson",
    data = without_fakespot_data
)
```

```{r}
reduced_model <- glm(
    search_rank ~
    log_best_seller_rank +
    scaled_relevance_score,
    family = "poisson",
    data = complete_regression_data
)
```

```{r}
ranked_coefficients <-
    summary(main_model)$coefficients %>%
    as.data.frame %>%
    mutate(coefficient = rownames(.)) %>%
    arrange(`Pr(>|z|)`) %>%
    filter(
        !startsWith(coefficient, "best_seller_category") &
        !startsWith(coefficient, "category") &
        !startsWith(coefficient, "department") &
        !startsWith(coefficient, "query") &
        !startsWith(coefficient, "unit")
    )
```

```{r}
raw_intervals <- confint.default(main_model)
```

```{r}
non_log_intervals <- ((exp(raw_intervals) - 1) * 100)
```

```{r}
hundred_intervals <- ((exp(raw_intervals * 100) - 1) * 100)
```

```{r}
negative_intervals <- -raw_intervals
```

```{r}
negative_non_log_intervals <- -non_log_intervals
```

```{r}
negative_hundred_intervals <- -hundred_intervals
```

```{r}
colnames(negative_intervals) <- c("97.5 %", "2.5 %")
```

```{r}
colnames(negative_non_log_intervals) <- c("97.5 %", "2.5 %")
```

```{r}
colnames(negative_hundred_intervals) <- c("97.5 %", "2.5 %")
```

```{r}
relevance_coefficient <- coef(main_model)[["scaled_relevance_score"]]
```

```{r}
exp_relevance_coefficient <- exp(relevance_coefficient)
```

```{r}
duplicates_data <-
    read_csv("results/duplicates_data.csv", show_col_types = FALSE) %>%
    arrange(query, page_number, page_rank) %>%
    # if there are multiple sponsored listings, use the first one
    # same with unsponsored listings
    group_by(query, ASIN, sponsored) %>%
    slice(1) %>%
    ungroup %>%
    # rerank over all pages
    arrange(query, page_number, page_rank) %>%
    group_by(query) %>%
    mutate(search_rank = seq_len(n())) %>%
    ungroup() %>%
    select(-page_number, -page_rank) %>%
    # reverse the order for the locf
    arrange(query, desc(search_rank)) %>%
    group_by(query) %>%
    mutate(
        # locf the unsponsored listings
        next_unsponsored_ASIN = 
            na.locf(ifelse(sponsored, NA, ASIN), na.rm = FALSE)
    ) %>%
    ungroup() %>%
    # return to forward ordering
    arrange(query, search_rank)
```

```{r}
unsponsored_duplicates_data <-
    duplicates_data %>%
    select(query, ASIN, search_rank, sponsored) %>%
    filter(!sponsored) %>%
    rerank %>%
    rename(unsponsored_rank = search_rank)
```

```{r}
sponsored_duplicates_data <-
    duplicates_data %>%
    select(query, ASIN, search_rank, sponsored) %>%
    pivot_wider(names_from = sponsored, values_from = search_rank) %>%
    rename(
        unsponsored_combined_rank = `FALSE`,
        sponsored_combined_rank = `TRUE`
    ) %>%
    filter(!is.na(sponsored_combined_rank)) %>%
    # add the unsponsored rank
    left_join(
        unsponsored_duplicates_data %>%
        select(
            query,
            ASIN,
            unsponsored_rank
        ),
        by = c("query", "ASIN")
    ) %>%
    # add the displaced ASIN
    left_join(
        duplicates_data %>%
        filter(sponsored) %>%
        select(
            query,
            ASIN,
            displaced_ASIN = next_unsponsored_ASIN
        ),
        by = c("query", "ASIN")
    ) %>%
    # use displaced ASIN to find the displaced rank
    left_join(
        unsponsored_duplicates_data %>%
        select(
            query,
            displaced_ASIN = ASIN,
            displaced_unsponsored_rank = unsponsored_rank
        ),
        by = c("query", "displaced_ASIN")
    ) %>%
    mutate(
        relevance_boost = 
            (log(displaced_unsponsored_rank) - log(unsponsored_rank)) /
            relevance_coefficient
    )
```

```{r}
percent_present <- function(vector) {
    sum(!is.na(vector)) / length(vector) * 100
}
```

```{r}
queries <- read_csv("inputs/queries.csv", show_col_types = FALSE)
```

```{r}
raw_robustness_coefficients <- c(
    `Main estimate` = coef(main_model)[["scaled_relevance_score"]],
    `Estimate with only statistically significant predictors` = 
        coef(significant_model)[["scaled_relevance_score"]],
    `Estimate without fakespot rating and more data` = 
        coef(without_fakespot_model)[["scaled_relevance_score"]],
    `Estimate predicting log rank with standard regression` = 
        coef(log_model)[["scaled_relevance_score"]],
    `Estimate with only the best seller rank and relevance scores` =
        coef(reduced_model)[["scaled_relevance_score"]]
)
```


```{r}
percent_present <- sum(!is.na(sponsored_duplicates_data$unsponsored_rank)) / nrow(sponsored_duplicates_data) * 100
```

```{r}
top_sponsored_data <-
    sponsored_duplicates_data %>%
    filter(displaced_unsponsored_rank <= 10)
```


I downloaded a list of the `r pander(nrow(queries))` most searched terms on Amazon in April 2023 from Amazon's data portal [TODO: better description].
I excluded terms for which the "Top Clicked Category" was a digital category, because shipping data is not applicable to digital products.
Here are the first 10:

```{r}
queries %>%
    rename(`Query` = query) %>%
    slice(1:10) %>%
    pander()
```

For each search, I collected the first page of results.
Then, I removed sponsored listings, because I assume the boost from sponsorship is not fixed.
For example, Amazon might always promote a sponsored listing to the first position, regardless of listing's organic rank.
Then, for each product, I removed duplicate listings within each search (keeping only the first listing).

For each product, I scraped the following variables from the product page:

- `amazon_brand`: Whether the Amazon owns the product's brand
- `amazons_choice`: Whether Amazon labeled the product "Amazon's choice".
- `answered_questions`: The number of answered questions. Amazon reports "1000+" if there are more than 1000 answers, in which case, I used 1000.
- `best_seller_category`: Amazon chooses a handful of categories containing the product, and reports the product's best-seller rank within each (see `log_best_seller_rank` and `category` below). For this variable, I used the most general category listed with a best seller rank. Note that this might not be the same as `category` below.
- `category`: Amazon categorizes products into a nested tree, with more general categories containing more specific categories. I used the most general category listed.
- `climate_friendly`: Whether Amazon labeled the product "Climate Pledge Friendly".
- `coupon_percent`: The coupon amount as a percentage of the price (or 0 if no coupon available).
- `department`: The department of the product. Departments are similar to categories, but there are fewer departments, and Amazon does not nest departments.
- `discount_percent` The discount amount as a percent of the (undiscounted) price (or 0 for no discount available).
- `fakespot_rating`: Fakespot rates products based on the amount of fake reviews Fakespot predicts a product has. Fakespot gives five ratings, in order: `A`, `B`, `C`, `D`, and `F`, with `A` for products Fakespot predicts have the fewest number of fake ratings. Fakespot only reports ratings for products with enough reviews to calculate a ranking.
- `free_returns`: Whether returning the product is free.
- `limited_stock`: Whether there is a limited stock of the product.
- `log_best_seller_rank`: The log of the product's rank within its `best_seller_category`, for example, `log(64)` for a product that is #64 in Watches
- `log_unit_price`: The log of the unit price, the price per unit of the product (or per purchase if Amazon did not specify a unit price). I excluded free products, so the unit price is always positive, and thus, its log always exists.
- `new_seller`: Fakespot reports whether the seller only recently started selling.
- `number_of_ratings`: The number of ratings.
- `rush_shipping_available`: Whether rush shipping is available.
- `ships_from_amazon`: Whether Amazon ships the product.
- `small_business`: Whether the seller has a small business badge.
- `sold_by_amazon`: Whether Amazon decides the price of the product.
- `standard_shipping_cost`: The cost for standard shipping.
- `standard_shipping_date_start`: The earliest arrival date for standard shipping.
- `standard_shipping_date_end`: The latest arrival date for standard shipping.
- `subscribe_coupon`: Whether a coupon is available only if you subscribe.
- `subscription_available`: Whether buyers can subscribe to purchase the product, for example, purchase a shipment each month.
- `download_time`: The time of day I downloaded the product page.
- `unit`: The units of the product, for example, ounces, or "purchase" if Amazon did not specify units.
- `one_star_percent`: The percentage of one-star reviews.
- `two_star_percent`: The percentage of two-star reviews.
- `four_star_percent`: The percentage of four-star reviews.
- `five_star_percent`: The percentage of five-star reviews.

I also used Apache Lucene to calculate a relevance scores.
For each product, I downloaded all visible text from the product page (excluding text in advertisements and unrelated widgets).
Then, I matched each query against the visible text on the product page.
For each match, Lucene returned a relvance score.
A higher relevance score corresponds to a product more relevant to the query.
This scores are difficult to interpret, because the formula Lucene uses to calculate relevance scores is complicated.
Thus, I standardized the scores, and used "standard deviations" as the relevance unit below.

There is a lot of missing data, for two reasons.
First, some variables might not always be applicable.
For example, Fakespot can only calculate a rating for products with a sizable number of reviews.
Second, I only parsed the most common formats of Amazon product pages, but some sellers format their product pages in unique ways.

Overall, there were `r pander(nrow(unsponsored_regression_data))` products.
There is a complete set of variables for `r pander(nrow(complete_regression_data))` products.
I excluded products with missing data, but I do not think this biased my results.
Then, I reranked the complete data.

Then, I used the variables above, and the relvance scores, to predict the search rank, with a fixed effect for each search term.
I used a Poisson GLM regression.
I used a Poisson distribution because the ranks are positive integers.
This is not a perfect fit because the ranks are not count data, but empirically, the distribution fits fairly well.
I tested the fit of this distribution with the quantile-quantile plot below.
Because most of the residuals lie along the diagonal line, this distribution fits the data fairly well.
I considered using random effects for the categorical variables, but when I combined the GLM with random effects, the estimates did not converge.
I also considered using a non-parametric "exploded logit" to predict the rankings, but again, the estimates did not converge.

```{r}
plot(main_model, which = 2)
```

Here are the coefficient interpretations for the 10 most statistically significant coefficients, from most to least, excluding those for categorical dummy variables.
In this context, statistical significance corresponds to predictive power.
All else being equal, with 95% confidence,

- When the product's best-seller rank becomes 1% closer to the top, Amazon ranks a product `r pander(raw_intervals["log_best_seller_rank", "2.5 %"])`% to `r pander(raw_intervals["log_best_seller_rank", "97.5 %"])`% closer to the top.
- When the relevance score increases by 1 standard deviation, Amazon ranks a product `r pander(negative_non_log_intervals["scaled_relevance_score", "2.5 %"])`% to `r pander(negative_non_log_intervals["scaled_relevance_score", "97.5 %"])`% closer to the top.
- When the number of answered questions increases by 1, Amazon ranks a product `r pander(non_log_intervals["answered_questions", "2.5 %"])`% to `r pander(non_log_intervals["answered_questions", "97.5 %"])`% further from the top.
- Amazon ranks a product from a small business `r pander(non_log_intervals["small_businessTRUE", "2.5 %"])`% to `r pander(non_log_intervals["small_businessTRUE", "97.5 %"])`% further from the top.
- When the number of ratings increases by 100, Amazon ranks a product `r pander(negative_hundred_intervals["number_of_ratings", "2.5 %"])`% to `r pander(negative_hundred_intervals["number_of_ratings", "97.5 %"])`% closer to the top.

Here are the coefficient interpretations for self-preferencing variables.
All else being equal, with 95% confidence,

- Amazon ranks an Amazon brand product `r pander(non_log_intervals["amazon_brandTRUE", "2.5 %"])`% to `r pander(non_log_intervals["amazon_brandTRUE", "97.5 %"])`% further from the top.
- Amazon ranks a product Amazon controls the price of ("sold by" Amazon) `r pander(negative_non_log_intervals["sold_by_amazonTRUE", "2.5 %"])`% to `r pander(negative_non_log_intervals["sold_by_amazonTRUE", "97.5 %"])`% closer to the top.
- Amazon ranks a product shipped by Amazon `r pander(negative_non_log_intervals["ships_from_amazonTRUE", "2.5 %"])`% to `r pander(negative_non_log_intervals["ships_from_amazonTRUE", "97.5 %"])`% closer to the top.

To test the robustness of 

```{r}
((exp(raw_robustness_coefficients) - 1) * 100) %>%
    as.list %>%
    pander()
```

```{r}
# Basic histogram
ggplot(sponsored_duplicates_data) +
    aes(x = relevance_boost) + 
    geom_histogram()
```

```{r}
ggplot(top_sponsored_data) +
    aes(x = relevance_boost, bin_width = 1) +
    geom_histogram(binwidth = 1.5)
```
